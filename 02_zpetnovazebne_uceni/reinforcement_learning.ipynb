{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zpětnovazební učení"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "V tomto cvičení budeme pracovat s knihovnou Gymnasium (udržovaný následovník knihovny Gym od OpenAI) - https://gymnasium.farama.org/, což je open source rozhraní určené pro úkoly zpětnovazebního učení. Jeho hlavní výhodou je, že implementace různých typů algoritmů pro zpětnovazební učení je v něm vcelku jednoduchá. Popis základních funkcí gymnasia se nachází v kódu níž.\n",
    "\n",
    "Dnešní úkol bude naimplementovat agenta, který se učí chovat v nějakém prostředí (konkrétně v MountainCar) pomocí Q-učení.\n",
    "\n",
    "Q-učení je způsob, kdy se agent učí svou strategii, jak se chovat v daném prostředí, pomocí zpětné vazby, kterou od prostředí za své chování dostává. Na rozdíl od hladového agenta (který jen v každém stavu vybírá nový stav na základě akce, co maximalizuje jeho užitek), bere v potaz to, že mezi stavy existují vztahy, které jsou dány Bellmanovými rovnicemi.\n",
    "\n",
    "Nyní se tedy podíváme na příklad autíčka, které se snaží dostat do cíle, ale pohybuje se náhodně."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "\n",
    "import gymnasium\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "\n",
    "import pygame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ukážeme si, jak si vytvořit jednoduché prostředí *MountainCar*: https://gymnasium.farama.org/environments/classic_control/mountain_car/\n",
    "\n",
    "Cílem je, aby se autíčko dostalo z údolí až nahoru k vlaječce. V této základní verzi je zde v každém stavu náhodně zvolena akce pro pohyb.\n",
    "\n",
    "(V následujícím kódu je zobrazování nastaveno tak, aby nám otevřelo prostředí v novém okně a v tom nám ukázalo běh agenta.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gymnasium.make(\"MountainCar-v0\", render_mode=\"human\")\n",
    "\n",
    "# Let's see, how the observation and action spaces of this environment look like\n",
    "print(\"Observation space:\", env.observation_space)\n",
    "print(\"Observation space - low:\", env.observation_space.low)\n",
    "print(\"Observation space - high:\", env.observation_space.high)\n",
    "print(\"Action space:\", env.action_space)\n",
    "\n",
    "terminated, truncated = False, False\n",
    "state, info = env.reset()\n",
    "while not terminated and not truncated:\n",
    "    state, reward, terminated, truncated, info = env.step(env.action_space.sample()) # Take a random action\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pokud bychom si prostředí chtěli vyzkoušet sami, dá se udělat třeba něco takového (není to sice zcela odladěné (hlavně nedržte víc kláves najednou), ale funguje to dostatečně):\n",
    "\n",
    "(A ano, poběží to dokola, dokud to nevypnete...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Pygame\n",
    "pygame.init()\n",
    "\n",
    "# Create a Pygame window\n",
    "screen = pygame.display.set_mode((640,  480))\n",
    "\n",
    "# Create the environment\n",
    "env = gymnasium.make(\"MountainCar-v0\", render_mode=\"human\")\n",
    "\n",
    "# Initialize variables\n",
    "env_terminated, done = False, False\n",
    "R = 0\n",
    "action_to_perform = 1\n",
    "\n",
    "# Reset the environment\n",
    "state = env.reset()\n",
    "\n",
    "# Define the mapping from keyboard keys to actions\n",
    "key_to_action = {\n",
    "    pygame.K_LEFT:   0,  # Move cart left\n",
    "    pygame.K_RIGHT:   2  # Move cart right\n",
    "}\n",
    "\n",
    "# Main loop\n",
    "while not env_terminated:\n",
    "    # Process Pygame events\n",
    "    for event in pygame.event.get():\n",
    "        if event.type == pygame.QUIT:\n",
    "            env_terminated = True\n",
    "            break\n",
    "            \n",
    "        elif event.type == pygame.KEYDOWN:\n",
    "            if event.key in key_to_action:\n",
    "                action = key_to_action[event.key]\n",
    "                action_to_perform = action\n",
    "\n",
    "        elif event.type == pygame.KEYUP:\n",
    "            action_to_perform = 1 # Don't do anything\n",
    "\n",
    "    if not env_terminated and not done:\n",
    "        state, reward, terminated, truncated, info = env.step(action_to_perform)\n",
    "        done = terminated or truncated\n",
    "        R += reward\n",
    "\n",
    "    if done:\n",
    "        print(\"Return:\", R)\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        R = 0\n",
    "\n",
    "    # Update the display\n",
    "    pygame.display.flip()\n",
    "\n",
    "# Clean up\n",
    "env.close()\n",
    "pygame.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vraťme se zpět k agentům. Zkusíme do kódu výše připsat obecnou třídu pro agenta, který se v prostředí chová náhodně. Lze ho později použít jako základ pro zpětnovazebního agenta. Stav agenta je pozice a rychlost, akce může být akcelerace vlevo, vpravo a nebo nicnedělání. Budeme opakovat několik iterací pro \"trénovaní\", kdy každá iterace for cyklu je jedna hra s novým náhodným začátkem a ve while cyklu se trénují přechody mezi stavy agenta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General class for a (random) agent\n",
    "class RandomAgent:\n",
    "    def __init__(self, action_space):\n",
    "        self.action_space = action_space\n",
    "    \n",
    "    def act(self, state):\n",
    "        return self.action_space.sample()\n",
    "    \n",
    "    def train(self, state, action, reward, next_state, terminated):\n",
    "        pass\n",
    "    \n",
    "\n",
    "env = gymnasium.make(\"MountainCar-v0\")\n",
    "agent = RandomAgent(env.action_space)\n",
    "\n",
    "# Training loop (the agent trains for the duration of hundred episodes)\n",
    "print(\"Training the agent...\")\n",
    "total_returns = []\n",
    "for _ in range(100):\n",
    "    observation, _ = env.reset()   \n",
    "    done = False\n",
    "    R = 0. # Cummulative reward (otherwise known as return) - just for logging purposes\n",
    "    \n",
    "    while not done:\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        agent.train(state, action, reward, next_state, terminated)\n",
    "        \n",
    "        state = next_state\n",
    "        done = terminated or truncated\n",
    "        R += reward\n",
    "        \n",
    "    total_returns.append(R)\n",
    "\n",
    "# Let's show a plot of the learning progression\n",
    "plt.plot(utils.moving_average(total_returns, 10))\n",
    "plt.show()\n",
    "\n",
    "# Evaluation\n",
    "print(\"Evaluating the trained agent...\")\n",
    "\n",
    "# We pass only the name of the environment (and eventual arguments), not the environment itself, because we don't want to render\n",
    "# anything during the training, but we want to do so during the test phase. This we achieve by creating a new environment\n",
    "# for the testing / simulation with properly set render_mode argument (which we do inside the simulate function).\n",
    "print(\"Obtained returns:\", utils.simulate(agent, \"MountainCar-v0\", steps=200, episodes=1))\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Úkol na cvičení\n",
    "\n",
    "Zkuste si místo náhodného agenta naprogramovat třídu agenta, který se učí chovat v prostředí MountainCar pomocí Q-učení. Dejte pozor na to, že prostředí vrací jako stav spojité hodnoty, takže je třeba si z nich nějak udělat prostředí diskrétní (tedy s konečným množstvím stavů).\n",
    "\n",
    "Níže je návrh obrysů takového agenta, který můžete případně použít."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAgent:\n",
    "    def __init__(self, action_space, epsilon=0.1, alpha=0.1, gamma=0.9):\n",
    "        self.action_space = action_space\n",
    "        \n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "        num_bins_position = 11 # subintervals for the position\n",
    "        num_bins_velocity = 6 # subintervals for the velocity\n",
    "\n",
    "        mean_pos = -0.5\n",
    "        std_pos = 0.3\n",
    "        min_pos = -1.2\n",
    "        max_pos = 0.6\n",
    "\n",
    "        low_prob = norm.cdf(min_pos, loc=mean_pos, scale=std_pos)\n",
    "        high_prob = norm.cdf(max_pos, loc=mean_pos, scale=std_pos)\n",
    "\n",
    "        # use normal distribution for bins position with expected value at the bottom of the valley and standard deviation such that the bins are not too wide\n",
    "        percentiles = np.linspace(low_prob, high_prob, num_bins_position + 1)\n",
    "        self.bins_position = norm.ppf(percentiles, loc=mean_pos, scale=std_pos)\n",
    "\n",
    "        self.bins_velocity = np.linspace(-0.07, 0.07, num_bins_velocity + 1)\n",
    "\n",
    "        self.Q = np.zeros((num_bins_position, num_bins_velocity, 3)) # Q[position, velocity, action]\n",
    "        \n",
    "        self.greedy = False\n",
    "        \n",
    "        \n",
    "    def _discretize(self, state):\n",
    "        state_position = np.digitize(state[0], self.bins_position) - 1\n",
    "        state_velocity = np.digitize(state[1], self.bins_velocity) - 1\n",
    "        return (state_position, state_velocity)\n",
    "    \n",
    "    \n",
    "    def act(self, state):\n",
    "        current_state = self._discretize(state)\n",
    "        \n",
    "        if not self.greedy and np.random.random() < self.epsilon:\n",
    "            chosen_action = self.action_space.sample()\n",
    "        else:\n",
    "            state_qs = self.Q[current_state[0], current_state[1], :]\n",
    "            max_q = np.max(state_qs)\n",
    "            # break ties randomly\n",
    "            best_actions = np.where(state_qs == max_q)[0]\n",
    "            chosen_action = np.random.choice(best_actions)\n",
    "\n",
    "        return chosen_action\n",
    "        \n",
    "        \n",
    "    def train(self, state, action, reward, next_state, terminated):\n",
    "        state, next_state = self._discretize(state), self._discretize(next_state)\n",
    "        \n",
    "        if not terminated:\n",
    "            rest_of_the_episode_estimation = np.max(self.Q[next_state[0], next_state[1], :])\n",
    "        else:\n",
    "            rest_of_the_episode_estimation = 0.\n",
    "        \n",
    "        self.Q[state[0], state[1], action] += \\\n",
    "            self.alpha * (reward + self.gamma * rest_of_the_episode_estimation - self.Q[state[0], state[1], action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gymnasium.make(\"MountainCar-v0\")\n",
    "agent = QAgent(env.action_space, epsilon=1, alpha=0.1, gamma=0.9)\n",
    "decay = 0.9\n",
    "\n",
    "# Training\n",
    "print(\"Training the agent...\")\n",
    "agent.greedy = False\n",
    "\n",
    "total_returns = []\n",
    "for _ in range(1000):\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    R = 0. # return\n",
    "    \n",
    "    while not done:\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        agent.train(state, action, reward, next_state, terminated)\n",
    "        \n",
    "        state = next_state\n",
    "        done = terminated or truncated\n",
    "        R += reward\n",
    "    \n",
    "    agent.epsilon *= decay # decay epsilon\n",
    "    total_returns.append(R)\n",
    "\n",
    "env.close()\n",
    "\n",
    "plt.plot(utils.moving_average(total_returns, 10))\n",
    "plt.show()\n",
    "\n",
    "# Evaluation\n",
    "print(\"Evaluating the trained agent...\")\n",
    "agent.greedy = True\n",
    "print(\"Obtained returns:\", utils.simulate(agent, \"MountainCar-v0\", steps=200, episodes=1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (PIA Course)",
   "language": "python",
   "name": "pia-course"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
